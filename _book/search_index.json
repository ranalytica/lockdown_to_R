[
["data-science-capstone.html", " 11 Data Science Capstone ", " 11 Data Science Capstone "],
["environment.html", "11.1 Environment", " 11.1 Environment enviro &lt;- c(&quot;rmdformats&quot;,&quot;knitr&quot;,&quot;rvest&quot;, &quot;tidytext&quot;, &quot;tm&quot;, &quot;tidyverse&quot;, &quot;NLP&quot;, &quot;knitr&quot;, &quot;readr&quot;, &quot;openNLP&quot;, &quot;glue&quot;, &quot;RWeka&quot;, &quot;stringi&quot;, &quot;SnowballC&quot;, &quot;wordcloud2&quot;) for (i in 1:length(enviro)){ library(enviro[i], character.only = TRUE) } ## Global options options(max.print=&quot;75&quot;) opts_chunk$set(cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) "],
["data-loading-and-summary.html", "11.2 Data Loading and Summary", " 11.2 Data Loading and Summary View Code ## Download file (run at the beginning only) --------- # if (!file.exists(&#39;data&#39;)) { dir.create(&#39;data&#39;) } # download.file( # &#39;https://d396qusza40orc.cloudfront.net/ds # scapstone/dataset/Coursera-SwiftKey.zip&#39;, destfile = # &#39;./data/Coursera-SwiftKey.zip&#39; ) # unzip(&#39;./data/Coursera-SwiftKey.zip&#39;, exdir = # &#39;./data&#39;) ## Combining ----------- files2 &lt;- list.dirs(&quot;./data/final&quot;) lsfile &lt;- paste0(files2[2:5], &quot;/&quot;, list.files(files2[2:5])) # gives us the directory path ldir &lt;- normalizePath(files2[2:5], &quot;rb&quot;) # gives us full path and filename finaldir &lt;- dir(path = ldir, full.names = TRUE) ## Build a table -------------- ## Num_Words total number of words in a txt file Num_Words &lt;- vector(&quot;numeric&quot;) ## Num_Lines number of lines per txt file Num_Lines &lt;- vector(&quot;numeric&quot;) ## Range of words per line Min_Words &lt;- vector(&quot;numeric&quot;) Mean_Words &lt;- vector(&quot;numeric&quot;) Max_Words &lt;- vector(&quot;numeric&quot;) for (i in 1:12) { Num_Words[i] &lt;- print(sum(stri_count_words(readLines(finaldir[[i]])))) Mean_Words[i] &lt;- print(round(mean(stri_count_words(readLines(finaldir[[i]])))), digits = 2) Min_Words[i] &lt;- print(round(min(stri_count_words(readLines(finaldir[[i]])))), digits = 2) Max_Words[i] &lt;- print(round(max(stri_count_words(readLines(finaldir[[i]])))), digits = 2) Num_Lines[i] &lt;- print(length(readLines(finaldir[i]))) } [1] 12682659 [1] 34 [1] 0 [1] 1638 [1] 371440 [1] 13375092 [1] 55 [1] 1 [1] 603 [1] 244743 [1] 11646033 [1] 12 [1] 0 [1] 42 [1] 947774 [1] 37546250 [1] 42 [1] 0 [1] 6726 [1] 899288 [1] 34762395 [1] 34 [1] 1 [1] 1796 [1] 1010242 [1] 30093372 [1] 13 [1] 1 [1] 47 [1] 2360148 [1] 12785318 [1] 29 [1] 0 [1] 2353 [1] 439785 [1] 10532432 [1] 22 [1] 1 [1] 478 [1] 485758 [1] 3147083 [1] 11 [1] 1 [1] 44 [1] 285214 [1] 9388482 [1] 28 [1] 1 [1] 1197 [1] 337100 [1] 9057248 [1] 46 [1] 1 [1] 1581 [1] 196360 [1] 9231328 [1] 10 [1] 1 [1] 36 [1] 881414 # Combine ------------- list_files &lt;- tibble(Name = list.files(files2[2:5]), Size_MB = round(file.size(finaldir)/10^6, digits = 2), Lines = Num_Lines, Words = Num_Words, Min = Min_Words, Average = Mean_Words, Max = Max_Words) # knit to table ----------- table1 &lt;- kable(list_files, align = c(rep(&quot;c&quot;, times = 5))) table1 Name Size_MB Lines Words Min Average Max de_DE.blogs.txt 85.46 371440 12682659 0 34 1638 de_DE.news.txt 95.59 244743 13375092 1 55 603 de_DE.twitter.txt 75.58 947774 11646033 0 12 42 en_US.blogs.txt 210.16 899288 37546250 0 42 6726 en_US.news.txt 205.81 1010242 34762395 1 34 1796 en_US.twitter.txt 167.11 2360148 30093372 1 13 47 fi_FI.blogs.txt 108.50 439785 12785318 0 29 2353 fi_FI.news.txt 94.23 485758 10532432 1 22 478 fi_FI.twitter.txt 25.33 285214 3147083 1 11 44 ru_RU.blogs.txt 116.86 337100 9388482 1 28 1197 ru_RU.news.txt 119.00 196360 9057248 1 46 1581 ru_RU.twitter.txt 105.18 881414 9231328 1 10 36 "],
["processing-data-for-tokenization.html", "11.3 Processing data for tokenization", " 11.3 Processing data for tokenization set.seed(420) source_data &lt;- DirSource(ldir) corpus_data &lt;- VCorpus(source_data) summary(corpus_data) Length Class Mode de_DE.blogs.txt 2 PlainTextDocument list de_DE.news.txt 2 PlainTextDocument list de_DE.twitter.txt 2 PlainTextDocument list en_US.blogs.txt 2 PlainTextDocument list en_US.news.txt 2 PlainTextDocument list en_US.twitter.txt 2 PlainTextDocument list fi_FI.blogs.txt 2 PlainTextDocument list fi_FI.news.txt 2 PlainTextDocument list fi_FI.twitter.txt 2 PlainTextDocument list ru_RU.blogs.txt 2 PlainTextDocument list ru_RU.news.txt 2 PlainTextDocument list ru_RU.twitter.txt 2 PlainTextDocument list inspect(corpus_data[5]) &lt;&lt;VCorpus&gt;&gt; Metadata: corpus specific: 0, document level (indexed): 0 Content: documents: 1 [[1]] &lt;&lt;PlainTextDocument&gt;&gt; Metadata: 7 Content: chars: 203223159 meta(corpus_data[[5]], &quot;id&quot;) [1] &quot;en_US.news.txt&quot; "],
["transforming-text.html", "11.4 Transforming Text", " 11.4 Transforming Text Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it. Profanity filtering —————— removeSpecialChars &lt;- function(x) gsub(&quot;[^a-zA-Z0-9 ]&quot;, &quot;&quot;, x) getTransformations() [1] &quot;removeNumbers&quot; &quot;removePunctuation&quot; [3] &quot;removeWords&quot; &quot;stemDocument&quot; [5] &quot;stripWhitespace&quot; us_news &lt;- tm_map(corpus_data[5], removePunctuation) us_news &lt;- tm_map(us_news, removeNumbers) us_news &lt;- tm_map(us_news, content_transformer(tolower)) us_news &lt;- tm_map(us_news, removeWords, stopwords(kind = &quot;en&quot;)) us_news &lt;- tm_map(us_news, stripWhitespace) us_news &lt;- tm_map(us_news, stemDocument) us_news &lt;- tm_map(us_news, content_transformer(removeSpecialChars)) us_news_trix &lt;- DocumentTermMatrix(us_news) inspect(us_news_trix) &lt;&lt;DocumentTermMatrix (documents: 1, terms: 249338)&gt;&gt; Non-/sparse entries: 249338/0 Sparsity : 0% Maximal term length: 93 Weighting : term frequency (tf) Sample : Terms Docs get like new one said say en_US.news.txt 60015 60466 70325 86725 250358 63085 Terms Docs state time will year en_US.news.txt 66073 65995 110914 109283 findFreqTerms(us_news_trix, 25) [1] &quot;aaa&quot; &quot;aah&quot; &quot;aardman&quot; [4] &quot;aaron&quot; &quot;aarp&quot; &quot;aau&quot; [7] &quot;aba&quot; &quot;aback&quot; &quot;abalon&quot; [10] &quot;abandon&quot; &quot;abat&quot; &quot;abba&quot; [13] &quot;abbey&quot; &quot;abbi&quot; &quot;abbott&quot; [16] &quot;abbottabad&quot; &quot;abbrevi&quot; &quot;abc&quot; [19] &quot;abdelkad&quot; &quot;abdic&quot; &quot;abdomen&quot; [22] &quot;abdomin&quot; &quot;abduct&quot; &quot;abdul&quot; [25] &quot;abduljabbar&quot; &quot;abdullah&quot; &quot;abdulmutallab&quot; [28] &quot;abdurraheem&quot; &quot;abe&quot; &quot;abel&quot; [31] &quot;abercrombi&quot; &quot;aberdeen&quot; &quot;aberr&quot; [34] &quot;abet&quot; &quot;abid&quot; &quot;abigail&quot; [37] &quot;abil&quot; &quot;abingdon&quot; &quot;abject&quot; [40] &quot;abl&quot; &quot;ablaz&quot; &quot;ablebodi&quot; [43] &quot;abli&quot; &quot;ablin&quot; &quot;abnorm&quot; [46] &quot;aboard&quot; &quot;abod&quot; &quot;abolish&quot; [49] &quot;abomin&quot; &quot;abort&quot; &quot;abound&quot; [52] &quot;aboutfac&quot; &quot;aboveaverag&quot; &quot;aboveground&quot; [55] &quot;abraham&quot; &quot;abram&quot; &quot;abras&quot; [58] &quot;abreast&quot; &quot;abreeya&quot; &quot;abreu&quot; [61] &quot;abroad&quot; &quot;abrupt&quot; &quot;absenc&quot; [64] &quot;absent&quot; &quot;absente&quot; &quot;absinth&quot; [67] &quot;absolut&quot; &quot;absolv&quot; &quot;absorb&quot; [70] &quot;absorpt&quot; &quot;abstain&quot; &quot;abstent&quot; [73] &quot;abstin&quot; &quot;abstract&quot; &quot;absurd&quot; [ reached getOption(&quot;max.print&quot;) -- omitted 25519 entries ] findAssocs(us_news_trix, &quot;feet&quot;, 0.1) $feet numeric(0) inspect(removeSparseTerms(us_news_trix, 0.4)) &lt;&lt;DocumentTermMatrix (documents: 1, terms: 249338)&gt;&gt; Non-/sparse entries: 249338/0 Sparsity : 0% Maximal term length: 93 Weighting : term frequency (tf) Sample : Terms Docs get like new one said say en_US.news.txt 60015 60466 70325 86725 250358 63085 Terms Docs state time will year en_US.news.txt 66073 65995 110914 109283 Terms(us_news_trix) [1] &quot;aaa&quot; [2] &quot;aaaa&quot; [3] &quot;aaaaa&quot; [4] &quot;aaaaaahh&quot; [5] &quot;aaaaaahhh&quot; [6] &quot;aaaaaahhhh&quot; [7] &quot;aaaaaand&quot; [8] &quot;aaaaahhhh&quot; [9] &quot;aaaaambival&quot; [10] &quot;aaaaarp&quot; [11] &quot;aaaah&quot; [12] &quot;aaaand&quot; [13] &quot;aaaextinguisher&quot; [14] &quot;aaagh&quot; [15] &quot;aaahhh&quot; [16] &quot;aaai&quot; [17] &quot;aaar&quot; [18] &quot;aaarat&quot; [19] &quot;aaarrgh&quot; [20] &quot;aaarrrgghhh&quot; [21] &quot;aab&quot; [22] &quot;aaba&quot; [23] &quot;aabor&quot; [24] &quot;aac&quot; [25] &quot;aacc&quot; [26] &quot;aaccedu&quot; [27] &quot;aacceduscienceobservatori&quot; [28] &quot;aaccord&quot; [29] &quot;aachen&quot; [30] &quot;aacom&quot; [31] &quot;aacountyorgag&quot; [32] &quot;aacr&quot; [33] &quot;aacu&quot; [34] &quot;aada&quot; [35] &quot;aaden&quot; [36] &quot;aadil&quot; [37] &quot;aadland&quot; [38] &quot;aadrman&quot; [39] &quot;aadvantag&quot; [40] &quot;aaf&quot; [41] &quot;aafedt&quot; [42] &quot;aafreen&quot; [43] &quot;aafter&quot; [44] &quot;aagaard&quot; [45] &quot;aah&quot; [46] &quot;aahh&quot; [47] &quot;aaholm&quot; [48] &quot;aahsorg&quot; [49] &quot;aahz&quot; [50] &quot;aai&quot; [51] &quot;aaii&quot; [52] &quot;aaiineogooglegroupscom&quot; [53] &quot;aaim&quot; [54] &quot;aak&quot; [55] &quot;aakash&quot; [56] &quot;aaker&quot; [57] &quot;aakhu&quot; [58] &quot;aal&quot; [59] &quot;aalam&quot; [60] &quot;aalber&quot; [61] &quot;aalborg&quot; [62] &quot;aalbu&quot; [63] &quot;aalder&quot; [64] &quot;aalesund&quot; [65] &quot;aali&quot; [66] &quot;aaliyah&quot; [67] &quot;aalok&quot; [68] &quot;aaltj&quot; [69] &quot;aalto&quot; [70] &quot;aam&quot; [71] &quot;aamc&quot; [72] &quot;aamco&quot; [73] &quot;aaminus&quot; [74] &quot;aampro&quot; [75] &quot;aan&quot; [ reached getOption(&quot;max.print&quot;) -- omitted 249263 entries ] freqq &lt;- colSums(as.matrix(us_news_trix)) "]
]
